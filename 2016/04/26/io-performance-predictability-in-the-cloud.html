<html><head>
<link rel=canonical href=https://blog.dbi-services.com/io-performance-predictability-in-the-cloud />
<meta name=description content='A blog post from 2016 about I/O Performance predictability in the Cloud'>
<title>I/O Performance predictability in the Cloud</title>
<link rel=stylesheet href=../../../style.css media=all>   

</head><body>

<p class=aboutme>
<br/>
<table width=100%>
<tr>
<td>
<a href=https://linkedin.com/in/franckpachot>Linkedin</a>, <a href=https://twitter.com/franckpachot>Twitter</a>, <a href=https://www.youtube.com/@franckpachot/community>Youtube</a>, <a href=https://mastodon.social/@FranckPachot>Mastodon</a>, <a href=https://dev.to/franckpachot>dev.to</a>
</td><td>
<img src=https://pbs.twimg.com/profile_images/1487359683119788033/4ejaJ4S5_400x400.jpg height=100 width=100/>
</td>
</tr>
</table>
<br/>
</p>

<p class=firstpub>This was first published on <a href=https://blog.dbi-services.com/io-performance-predictability-in-the-cloud>https://blog.dbi-services.com/io-performance-predictability-in-the-cloud</a> (2016-04-26), republishing for new followers. The content is related to the the versions available at the publication date</p>
								<h1 class="entry-title">I/O Performance predictability in the Cloud</h1>
		<div class="content-inner">
			
						
						
		   
			<p>You don&#8217;t need good performance only for your system. You need reliable performance. You need performance predictability.
<span id="more-8259"></span>
In a previous post I&#8217;ve measured physical IO performances of the Oracle Cloud Service, with SLOB. And performance is very good. And I continue to run the following SLOB configuration:
<pre>
UPDATE_PCT=25
RUN_TIME=3600
WORK_LOOP=100000
SCALE=90000M
WORK_UNIT=64
REDO_STRESS=LITE
LOAD_PARALLEL_DEGREE=4
</pre>
This is physical I/O (I&#8217;ve a small buffer cache here) with a fixed workload (the WORK_LOOP). When you do that, you should follow Kevin&#8217;s recommandation to set RUN_TIME to an arbitrary high value so that the run stops only at the end of WORK_LOOP. I thought one hour would always be sufficient but you will see that I was wrong.</p>
<p>For days, the performance was constant: 15 minutes to do about 6000 physical reads:</p>
<p><a href="http://blog.dbi-services.com/wp-insides/uploads/sites/2/2016/04/CaptureCLOUDIOPB1002.png" rel="lightbox[8259]"><img src="../wp-insides/uploads/sites/2/2016/04/CaptureCLOUDIOPB1002.png" alt="CaptureCLOUDIOPB1002" width="1638" height="166" class="alignnone size-full wp-image-8261" /></a></p>
<p>Except that we see something different on last Sunday. Let&#8217;s zoom at it (I&#8217;m using Orachrome Lighty):</p>
<p><a href="http://blog.dbi-services.com/wp-insides/uploads/sites/2/2016/04/CaptureCLOUDIOPB1001.png" rel="lightbox[8259]"><img src="../wp-insides/uploads/sites/2/2016/04/CaptureCLOUDIOPB1001.png" alt="CaptureCLOUDIOPB1001" width="1629" height="759" class="alignnone size-full wp-image-8260" /></a></p>
<p>As you see, the run has been longer, starting from April 23rd around 5pm and with the run time increasing. On 24th,from 3am to 10am you don&#8217;t see it increasing because it went over the 3600 seconds I&#8217;ve set in RUN_TIME. Then it came back to normal after 2pm.</p>
<p>In the lower part, you can see the plots from the Lighty ASH that shows an increase of I/O latency from 5am until noon.</p>
<p>As we don&#8217;t see the whole picture because the long run timed out, I extracted the physical reads per second from the AWR shapshots.</p>
<p><a href="http://blog.dbi-services.com/wp-insides/uploads/sites/2/2016/04/CaptureCLOUDIOPB1003.png" rel="lightbox[8259]"><img src="../wp-insides/uploads/sites/2/2016/04/CaptureCLOUDIOPB1003.png" alt="CaptureCLOUDIOPB1003" width="976" height="581" class="alignnone size-full wp-image-8277" /></a></p>
<p>Here are the raw values:</p>
<p><pre>
SQL&gt; set pagesize 1000 linesize 1000
column begin_interval_time format a17 trunc
column end_interval_time format a17 trunc
alter session set nls_timestamp_format='dd-mon-yyyy hh24:mi';
select * from (
select begin_interval_time,end_interval_time
,value-lag(value)over(partition by dbid,instance_number,startup_time order by snap_id) physical_reads
,round(((value-lag(value)over(partition by dbid,instance_number,startup_time order by snap_id))
 / (cast(end_interval_time as date)-cast(begin_interval_time as date)))/24/60/60) physical_reads_per_sec
, 24*60*60*((cast(end_interval_time as date)-cast(begin_interval_time as date))) seconds
from dba_hist_sysstat join dba_hist_snapshot using(dbid,instance_number,snap_id)
where stat_name='physical reads'
) where physical_reads&gt;0
/

BEGIN_INTERVAL_TI END_INTERVAL_TIME PHYSICAL_READS PHYSICAL_READS_PER_SEC    SECONDS
----------------- ----------------- -------------- ---------------------- ----------
23-apr-2016 10:17 23-apr-2016 10:34        6385274                   6272       1018
23-apr-2016 10:44 23-apr-2016 11:01        6388349                   6282       1017
23-apr-2016 11:12 23-apr-2016 11:29        6385646                   6316       1011
23-apr-2016 11:39 23-apr-2016 11:56        6386464                   6523        979
23-apr-2016 12:07 23-apr-2016 12:22        6387231                   6780        942
23-apr-2016 12:33 23-apr-2016 12:48        6386537                   7120        897
23-apr-2016 12:59 23-apr-2016 13:14        6389581                   7147        894
23-apr-2016 13:24 23-apr-2016 13:40        6388724                   6669        958
23-apr-2016 13:51 23-apr-2016 14:08        6387493                   6478        986
23-apr-2016 14:18 23-apr-2016 14:35        6386402                   6280       1017
23-apr-2016 14:46 23-apr-2016 15:03        6387153                   6293       1015
23-apr-2016 15:14 23-apr-2016 15:30        6386722                   6317       1011
23-apr-2016 15:41 23-apr-2016 15:58        6386488                   6374       1002
23-apr-2016 16:09 23-apr-2016 16:25        6387662                   6505        982
23-apr-2016 16:36 23-apr-2016 16:51        6387735                   6745        947
23-apr-2016 17:02 23-apr-2016 17:17        6387303                   7066        904
23-apr-2016 17:28 23-apr-2016 17:43        6387304                   7042        907
23-apr-2016 17:54 23-apr-2016 18:10        6388075                   6620        965
23-apr-2016 18:21 23-apr-2016 18:38        6386803                   6219       1027
23-apr-2016 18:48 23-apr-2016 19:06        6387318                   5969       1070
23-apr-2016 19:17 23-apr-2016 19:35        6386298                   5785       1104
23-apr-2016 19:46 23-apr-2016 20:05        6388856                   5517       1158
23-apr-2016 20:16 23-apr-2016 20:36        6387658                   5297       1206
23-apr-2016 20:47 23-apr-2016 21:09        6386522                   4838       1320
23-apr-2016 21:20 23-apr-2016 21:43        6386627                   4555       1402
23-apr-2016 21:54 23-apr-2016 22:17        6387922                   4521       1413
23-apr-2016 22:28 23-apr-2016 22:54        6388141                   4135       1545
23-apr-2016 23:04 23-apr-2016 23:32        6388043                   3905       1636
23-apr-2016 23:42 24-apr-2016 00:11        6392048                   3771       1695
24-apr-2016 00:21 24-apr-2016 00:54        6387294                   3237       1973
24-apr-2016 01:05 24-apr-2016 01:47        6391392                   2506       2550
24-apr-2016 01:58 24-apr-2016 02:49        6389860                   2102       3040
24-apr-2016 03:00 24-apr-2016 04:00        5723619                   1589       3602
24-apr-2016 04:10 24-apr-2016 05:00        3196078                   1073       2980
24-apr-2016 05:00 24-apr-2016 05:10         258522                    416        622
24-apr-2016 05:21 24-apr-2016 06:00        1308239                    564       2321
24-apr-2016 06:00 24-apr-2016 06:21        1246742                    973       1281
24-apr-2016 06:32 24-apr-2016 07:32        2743521                    762       3602
24-apr-2016 07:43 24-apr-2016 08:43        3498613                    971       3602
24-apr-2016 08:53 24-apr-2016 09:53        4207757                   1168       3603
24-apr-2016 10:04 24-apr-2016 11:00        5884053                   1764       3335
24-apr-2016 11:00 24-apr-2016 11:04         507668                   1960        259
24-apr-2016 11:15 24-apr-2016 12:00        6392338                   2371       2696
24-apr-2016 12:10 24-apr-2016 12:38        6387428                   3841       1663
24-apr-2016 12:49 24-apr-2016 13:11        6392759                   4742       1348
24-apr-2016 13:22 24-apr-2016 13:42        6387570                   5244       1218
24-apr-2016 13:53 24-apr-2016 14:12        6397352                   5707       1121
24-apr-2016 14:23 24-apr-2016 14:41        6389321                   5916       1080
24-apr-2016 14:51 24-apr-2016 15:09        6391483                   6070       1053
24-apr-2016 15:20 24-apr-2016 15:37        6385094                   6205       1029
24-apr-2016 15:47 24-apr-2016 16:04        6386833                   6342       1007
</pre></p>
<p>However, don&#8217;t think that performance was bad then. You have disks and average single block read in in few milliseconds:</p>
<p><pre>
Top 10 Foreground Events by Total Wait Time
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
                                           Total Wait       Wait   % DB Wait
Event                                Waits Time (sec)    Avg(ms)   time Class
------------------------------ ----------- ---------- ---------- ------ --------
db file sequential read             74,045      446.5       6.03   71.7 User I/O
db file parallel read                3,010      170.2      56.55   27.3 User I/O
DB CPU                                           10.6               1.7
log file sync                            3          0       0.79     .0 Commit
Disk file operations I/O                 5          0       0.14     .0 User I/O
utl_file I/O                             4          0       0.02     .0 User I/O
SQL*Net message to client                6          0       0.00     .0 Network
db file async I/O submit                 0          0                .0 System I
reliable message                         0          0                .0 Other
db file single write                     0          0                .0 User I/O
</pre></p>
<p>And the wait event histograms shows that only few I/O calls were above 32 milliseconds at the time of the worst IOPS:</p>
<p><pre>
                                                 % of Total Waits
                                 -----------------------------------------------
                           Waits
                           64ms
Event                      to 2s &lt;32ms &lt;64ms &lt;1/8s &lt;1/4s &lt;1/2s   &lt;1s   =2s
------------------------- ------ ----- ----- ----- ----- ----- ----- ----- -----
Data file init write          12  20.0  20.0  40.0  20.0
Disk file operations I/O       5  93.6   5.1                     1.3
control file sequential r      1 100.0                .0
db file async I/O submit      27  96.7    .9   1.1    .7    .4    .1    .1
db file parallel read       1487  50.6  24.0  13.8   8.3   3.4
db file parallel write       853  74.8  10.0   8.4   5.9    .9    .0
db file sequential read     2661  96.5   1.6   1.1    .6    .2    .0
</pre></p>
<p>Here is the wait event histogram at microsecond level at the time where the storage head cache hit was probably at its minimum.</p>
<p><pre>
06:21:48 SQL&gt;
EVENT                          WAIT_TIME_MICRO WAIT_COUNT WAIT_TIME_FORMAT
------------------------------ --------------- ---------- ------------------------------
db file sequential read                      1          0 1 microsecond
db file sequential read                      2          0 2 microseconds
db file sequential read                      4          0 4 microseconds
db file sequential read                      8          0 8 microseconds
db file sequential read                     16          0 16 microseconds
db file sequential read                     32          0 32 microseconds
db file sequential read                     64          0 64 microseconds
db file sequential read                    128          0 128 microseconds
db file sequential read                    256      16587 256 microseconds
db file sequential read                    512     340140 512 microseconds
db file sequential read                   1024      56516 1 millisecond
db file sequential read                   2048       5140 2 milliseconds
db file sequential read                   4096      12525 4 milliseconds
db file sequential read                   8192      45465 8 milliseconds
db file sequential read                  16384      53552 16 milliseconds
db file sequential read                  32768      14962 32 milliseconds
db file sequential read                  65536       9603 65 milliseconds
db file sequential read                 131072       4430 131 milliseconds
db file sequential read                 262144       1198 262 milliseconds
db file sequential read                 524288        253 524 milliseconds
db file sequential read                1048576          3 1 second
</pre></p>
<p>Those are the performances that you can expect from a busy load when you don&#8217;t hit the storage head cache. It&#8217;s not bad on average. This is just what you can expect from disk.
You just need to keep in mind that the amazing performances that you can see usually are not guaranteed. It&#8217;s very nice to get those performance for development or test environments, but do not rely on it.</p>
<table class="rw-rating-table rw-ltr rw-left rw-no-labels"><tr><td><nobr>&nbsp;</nobr></td><td><div class="rw-left"><div class="rw-ui-container rw-class-blog-post rw-urid-82600" data-img="http://blog.dbi-services.com/wp-insides/uploads/sites/2/2016/04/CaptureCLOUDIOPB1002.png"></div></div></td></tr></table>							
		</div><!--/content-inner-->
<div class="comment-wrap ">

	<h3 id="comments"> 2 Comments</h3>

	<div class="navigation">
		<div class="alignleft"></div>
		<div class="alignright"></div>
	</div>

	<ul class="comment-list ">
				<li class="comment even thread-even depth-1" id="comment-3402">
				<div id="div-comment-3402" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/43849a0f9407f04e67f24ae555f9c2ae?s=60&amp;d=https%3A%2F%2Fsecure.gravatar.com%2Favatar%2Fad516503a11cd5ca435acc9bb6523536%3Fs%3D60&amp;r=G' class='avatar avatar-60 photo' height='60' width='60' />			<cite class="fn"><a href='http://kevinclosson.net' rel='external nofollow' class='url'>Kevin</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="index.html#comment-3402">
			April 26, 2016 at 20 h 34 min</a>		</div>

		<p>Hi Franck,</p>
<p>   I think latencies as bad as 65ms to 3000ms are more likely a case where the VM is getting starved for CPU or swapped out perhaps. I can&#8217;t imagine any storage device taking that long.  Just a thought.
<table class="rw-rating-table rw-ltr rw-left rw-no-labels">
<tr>
<td><nobr>&nbsp;</nobr></td>
<td>
<div class="rw-left">
<div class="rw-ui-container rw-class-comment rw-urid-34031"></div>
</div>
</td>
</tr>
</table>

		<div class="reply"><a class='comment-reply-link' href='index.html#comment-3402' onclick='return addComment.moveForm( "div-comment-3402", "3402", "respond", "8259" )' aria-label='Reply to Kevin to Kevin'>Reply to Kevin</a></div>
				</div>
		</li><!-- #comment-## -->
		<li class="comment byuser comment-author-franck-pachot bypostauthor odd alt thread-odd thread-alt depth-1" id="comment-3403">
				<div id="div-comment-3403" class="comment-body">
				<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/9c04a89267afa42e63eeb3d620f4b873?s=60&amp;d=https%3A%2F%2Fsecure.gravatar.com%2Favatar%2Fad516503a11cd5ca435acc9bb6523536%3Fs%3D60&amp;r=G' class='avatar avatar-60 photo' height='60' width='60' />			<cite class="fn"><a href='https://www.linkedin.com/in/franckpachot' rel='external nofollow' class='url'>Franck Pachot</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="index.html#comment-3403">
			April 26, 2016 at 20 h 46 min</a>		</div>

		<p>Hi Kevin. Difficult to check that as I had very low CPU usage. I should run SLOB LIO in the same machine as PIO, but I don&#8217;t  know when this will happen again&#8230;
<table class="rw-rating-table rw-ltr rw-left rw-no-labels">
<tr>
<td><nobr>&nbsp;</nobr></td>
<td>
<div class="rw-left">
<div class="rw-ui-container rw-class-comment rw-urid-34041"></div>
</div>
</td>
</tr>
</table>

		<div class="reply"><a class='comment-reply-link' href='index.html#comment-3403' onclick='return addComment.moveForm( "div-comment-3403", "3403", "respond", "8259" )' aria-label='Reply to Franck to Franck Pachot'>Reply to Franck</a></div>
				</div>
		</li><!-- #comment-## -->
	</ul>

 

								<div id="respond" class="comment-respond">
</body></html>
